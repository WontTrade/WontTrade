# WontTrade LLM Trading System Specification

## 1. Scope and Objectives
- Deliver an autonomous, zero-interaction trading daemon implemented in Python 3.14t, managed via `uv`.
- Operate on Hyperliquid perpetual markets (BTC and ETH are primary) using the official Hyperliquid Python SDK, with first-class support for both mainnet and testnet clusters.
- Produce absolute target positions on each invocation; avoid sequencing instructions.
- Survive unexpected interruptions by reconciling state on restart without manual input.
- Delegate portfolio decisions to an OpenAI-hosted large language model; the engine applies the targets directly without post-processing risk filters. The model must always supply stop-loss, take-profit, and an explicit invalidation condition for the overall plan.
- Never trigger full liquidation on the user's behalf; the engine only aligns to model targets within validated constraints.
- Enforce formatting and linting through Ruff to maintain consistent code quality.

## 2. Core Constraints
- Runtime may stop at any moment; the next loop must bootstrap entirely from live snapshots.
- No interactive controls once configuration (AK/SK, risk limits, prompt template) is supplied.
- Every loop outputs the desired portfolio state for all tracked symbols, including explicit zeros.
- The official Hyperliquid SDK already exposes all required REST and WebSocket endpoints; no custom API clients.

## 3. Architecture Overview
```
bootstrap → event loop:
  loadSnapshot → enrichIndicators → buildLLMContext (includes prior decision)
  → callLLM (evaluate prior plan) → reconcilePositions → executeOrders → emitReports
```

### 3.1 Module Responsibilities
- `StateLoader`: Collect fresh market data (price, funding, OI, OHLCV) and account state (balances, positions, open orders) through the Hyperliquid SDK.
- `IndicatorEngine`: Calculate rolling EMA, MACD, RSI, ATR and other signals directly on the in-memory window required for prompting.
- `ContextBuilder`: Assemble a chronological prompt payload containing market snapshots, account posture, and metadata (Sharpe, uptime, invocation count).
- `LLMDecisionEngine`: Invoke OpenAI's Python SDK with deterministic temperature settings and strict response schema hints. Enforces presence of stop-loss / take-profit per symbol and a top-level invalidation condition.
- `PositionReconciler`: Compare current positions with the target portfolio to derive delta orders, ensuring idempotency and protection order alignment.
- `ExecutionService`: Submit create/cancel/modify requests through the Hyperliquid SDK, manage idempotency keys, and surface execution results.
- `AuditSink`: Persist structured logs, decision traces, and metrics for observability and post-mortem analysis.

## 4. Data Contracts
- `MarketSnapshot`: Timestamped mid-price series, indicator window (latest N points), funding/open-interest aggregates, volume context.
- `AccountSnapshot`: Cash balance, per-symbol positions (quantity, entry price, liquidation price, PnL), protection orders, leverage, Sharpe metrics.
- `TargetPosition`: `{symbol, targetSize, stopLoss, takeProfit, rationale, confidence}` generated by the LLM.
- `ExecutionPlan`: Minimal set of create/update/cancel actions derived from current vs. target positions.
All interfaces are Python dataclasses (or TypedDicts) shared across modules to avoid data clumps and implicit coupling.

## 5. Execution Loop Details
1. **Snapshot Load**  
   - Hyperliquid SDK WebSocket streams supply rolling OHLCV updates; REST fallbacks refresh balances and orders.  
   - On fresh startup, the loop blocks until both market and account snapshots are complete.
2. **Indicator Enrichment**  
   - Sliding-window buffers compute EMA/MACD/RSI/ATR per symbol.  
   - Persist only the history length required by the prompt to prevent redundant memory use.
3. **LLM Prompt Construction**  
   - Follow the baseline prompt structure provided in the reference, enforcing oldest→newest ordering.  
   - Embed uptime, invocation count, Sharpe, and any missing-stop alerts so the model has full situational awareness.
4. **Decision Inference**  
   - OpenAI Python SDK call with streaming disabled, bounded latency, and automatic retries on transient faults.  
   - Expected response: JSON array of `TargetPosition`; reject free-form prose.
5. **Decision Evaluation**  
   - Provide the previous explanation/condition to the LLM; if invalidation triggers, adjust targets, otherwise keep positions constant.  
   - Require every target to contain stop-loss、take-profit、confidence、rationale，并返回整体无效条件。  
6. **Plan Synthesis and Execution**  
   - Calculate per-symbol deltas (buy/sell quantity, stop, take-profit adjustments).  
   - Submit orders using the Hyperliquid SDK with deterministic client order IDs for idempotency.  
   - Ensure missing protective orders are added when the target includes them; absence of targets leaves existing protection untouched.
7. **Reporting**  
   - Append execution outcomes and model confidences to `decision-log.ndjson`.  
   - Update heartbeat file (timestamp, status, error codes) for external watchdogs.

## 6. LLM Integration
- Support both OpenAI and Azure OpenAI providers through the official Python SDKs; provider selection, credentials, and deployment metadata are defined in `wonttrade.toml`.  
- Maintain versioned prompt templates stored on disk; the loop reads but never mutates them.  
- Enforce JSON responses containing `explanation`, `invalidation_condition`, and a `targets` array where each entry specifies `symbol`, `target_size`, `stop_loss`, `take_profit`, `confidence`, `rationale`, and optional `margin`.  
- Log raw prompt and response for replay while redacting secrets, including the validation of prior decisions.  
- Provide optional temperature tuning and max token controls through static configuration.

## 7. Configuration and Secrets
- All runtime inputs (credentials, symbols, risk limits, runtime mode, telemetry paths) are sourced from an immutable TOML file (default `wonttrade.toml`). The daemon rejects missing or malformed fields rather than falling back to environment variables.  
- LLM credentials support OpenAI (`credentials.openai_api_key`) or Azure OpenAI (`credentials.azure_openai_api_key`) along with provider metadata (`llm.provider`, optional `[llm.azure]` table).
- Secrets load once at bootstrap; no hot reloading.  
- Optionally support configuration hashing to detect accidental edits at startup.
- Ruff enforces lint/format during CI and local development; integrate with task runners invoked through `uv`.

## 8. Observability and Resilience
- Structured logs (JSON Lines) tagged by `market`, `decision`, `execution`, `risk`, `health`.  
- Metrics output compatible with Prometheus text exposition (latency, execution success rates).  
- Heartbeat artifact updated every loop with last success timestamp and summary status.  
- Automatic retries with exponential backoff for Hyperliquid SDK operations; after configurable consecutive failures, pause new LLM calls but leave positions untouched.

## 9. Restart and Recovery Behavior
- At startup the engine immediately fetches `AccountSnapshot` and reconstructs the live posture before any LLM call.  
- Since decisions are absolute targets, the first reconciliation after a crash aligns actual holdings with the latest LLM recommendation.  
- Pending protective orders are re-synced: missing stop/take-profit orders are reinstated based on stored metadata or latest model output.

## 10. Future Extensions
- Expand prompt context with additional markets or macro indicators.  
- Introduce automated scenario testing that replays historical `MarketSnapshot` data through the same loop.  
- Add optional human-in-the-loop approval mode without breaking the zero-interaction baseline.  
- Support multiple concurrent LLM models with weighted blending after sufficient evaluation.

## 11. Backtesting Support
- Reuse the production event loop with a configurable `RuntimeMode` flag (`LIVE`, `TESTNET`, `BACKTEST`) so reconciliation and telemetry behave identically across modes.
- Provide a `BacktestReplayProvider` that streams Hyperliquid historical candles and funding (via the Info API) into `MarketSnapshot` objects, computing required indicators on the fly to match the real-time feature set.
- Replace live execution with a `SimulatedExecutor` that consumes `ExecutionPlan` actions, applies configurable slippage and fee models, maintains virtual balances, and emits `AccountSnapshot` updates for the next loop iteration.
- Always invoke the OpenAI LLM in real time during backtests; caching is forbidden. Each prompt/response pair must be logged with timestamps, model ID, and checksum for auditing and reproducibility.
- Emit dedicated artifacts at the end of a backtest (equity curve, drawdown series, latency metrics) while keeping per-iteration traces in `backtest-results.ndjson` for downstream analytics. Track whether the LLM reused or replaced the prior decision for later analysis.
